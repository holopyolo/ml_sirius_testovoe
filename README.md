# README for Flask Application with RAG Model 

## Обзор

Этот репозиторий содержит приложение Flask, которое использует модель расширенной генерации (RAG) для ответа на вопросы на основе предоставленного длинного контекста. Приложение предназначено для запуска локально или в контейнере Docker. ВАЖНО: Если проблемы с образом на докер хаб, пожалуйста соберите именно с этого гитхаб репозитория, поскольку могут быть проблемы с совместимостью куды(индекс ллама виновна, не успею переписать)
[DOCKER HUB](https://hub.docker.com/r/asdawdsasd/mlsiriustestovoe2)
## Способ решения

- На основе предоставленного вопроса генерируются похожие 2-3
- Получаем сота ретривером ранжированные чанки для каждого вопроса
- Реранжируем в общий список
- Подаем в ллм
- ps.: тестировал метод hybrid search c keyword ретривером в research.ipynb, но это не дало существенного прироста по сравнению с расширением запроса, также не применял few-shot поскольку incontext лернинг только портил качество, тут скорее стоит делать упор на качественную ретривер систему
## Research.ipynb

Файл содержит исследование помимо построения RAG (какой бенчмарк выбрать, какую метрику стоит использовать: сравним корреляции с эталоном гпт4мини и т.д)

## Установка

### Клонируйте

git clone <repository-url>
cd <repository-directory>


### Установите зависимости


pip install -r requirements.txt


### Используя докер


docker build -t flask-rag-app .


## Конфигурация

### Command Line Arguments

Вы можете настроить приложение, используя следующие аргументы командной строки:

- --llm: указать используемую языковую модель (по умолчанию: IlyaGusev/saiga_llama3_8b)
- --embedder: укажите используемую модель внедрения (по умолчанию: deepvk/USER-bge-m3)

### Запуск


python app.py



## API документация

### Endpoint

- POST /submit

### Запрос

Отправьте post запрос с таким json

{
    "context": "ваш контекст.",
    "question": "ваш запрос."
}


### Response

Ответ будет в таком формате:

{
    "Answer": "ответ на запрос."
}

